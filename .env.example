# LLM Configuration
# LLM 모델 이름 (예: llama3.2, mistral, codellama 등)
LLM_MODEL=llama3.2

# LLM API Base URL
# 로컬 LLM 서버: http://localhost:11434/v1
# 사용자 정의 포트를 사용하는 경우: http://localhost:YOUR_PORT/v1
LLM_BASE_URL=http://localhost:11434/v1

# Temperature (0.0 - 2.0)
# 낮을수록 결정론적, 높을수록 창의적
TEMPERATURE=0.7

# MCP Server Configuration
# MCP 서버 베이스 URL (mcp-servers 프로젝트)
MCP_BASE_URL=https://localhost:22000

# MCP SSL 검증 (개발 환경에서는 false 사용)
MCP_VERIFY_SSL=false

# MCP Tools 사용 여부 (true/false)
USE_MCP_TOOLS=true

# API Server Configuration
# API 서버 호스트 (0.0.0.0 = 모든 인터페이스에서 접근 가능)
API_HOST=0.0.0.0

# API 서버 포트
API_PORT=23000

# Langfuse Configuration (LLM Observability)
# Langfuse 사용 여부 (true/false)
LANGFUSE_ENABLED=false

# Langfuse 서버 주소
LANGFUSE_BASE_URL=http://192.168.3.20:3000

# Langfuse 공개 키 (선택 사항 - Langfuse 서버에서 발급)
LANGFUSE_PUBLIC_KEY=

# Langfuse 비밀 키 (선택 사항 - Langfuse 서버에서 발급)
LANGFUSE_SECRET_KEY=

# 환경 구분 (development, staging, production)
LANGFUSE_TRACING_ENVIRONMENT=development
