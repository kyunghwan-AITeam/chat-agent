# Ollama Configuration
# Ollama 모델 이름 (예: llama3.2, mistral, codellama 등)
OLLAMA_MODEL=llama3.2

# Ollama API Base URL
# 로컬 Ollama 서버: http://localhost:11434/v1
# 사용자 정의 포트를 사용하는 경우: http://localhost:YOUR_PORT/v1
OLLAMA_BASE_URL=http://localhost:11434/v1

# Temperature (0.0 - 2.0)
# 낮을수록 결정론적, 높을수록 창의적
TEMPERATURE=0.7

# MCP Server Configuration
# MCP 서버 베이스 URL (mcp-servers 프로젝트)
MCP_BASE_URL=https://localhost:22000

# MCP SSL 검증 (개발 환경에서는 false 사용)
MCP_VERIFY_SSL=false

# MCP Tools 사용 여부 (true/false)
USE_MCP_TOOLS=true
