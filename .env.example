# Ollama Configuration
# Ollama 모델 이름 (예: llama3.2, mistral, codellama 등)
OLLAMA_MODEL=llama3.2

# Ollama API Base URL
# 로컬 Ollama 서버: http://localhost:11434/v1
# 사용자 정의 포트를 사용하는 경우: http://localhost:YOUR_PORT/v1
OLLAMA_BASE_URL=http://localhost:11434/v1

# Temperature (0.0 - 2.0)
# 낮을수록 결정론적, 높을수록 창의적
TEMPERATURE=0.7
